# Quickstart

在这个快速入门指南中，我们将向您介绍如何：

- 设置并开始使用 LangChain、LangSmith 和 LangServe。
- 使用 LangChain 最基础和常用的组件，包括提示模板、模型和输出解析器。
- 应用 LangChain 表达式语言，这是构建 LangChain 的基础协议，它促进了组件间的链接。
- 利用 LangChain 构建一个简单的应用程序。
- 使用 LangSmith 对您的应用程序进行跟踪。
- 使用 LangServe 将您的应用部署为 REST API。

## 配置

### 环境

使用 LangChain 通常需要与一个或多个模型提供者、数据存储、API 等进行集成。在这个示例中，我们将使用 OpenAI 的模型 API。

### LangSmith

在使用 LangChain 构建的应用程序中，通常包含多个步骤和多次调用大语言模型（LLM）。随着应用程序变得越来越复杂，能够准确了解链或智能体内部发生的情况变得非常重要。最佳的方式是使用 LangSmith。

需要注意的是，虽然 LangSmith 并非必需，但它非常有用。如果您打算使用 LangSmith，在上述链接注册后，请确保设置您的环境变量以开始记录跟踪信息。

### LangServe

LangServe 帮助开发者将 LangChain 链部署为 REST API。虽然使用 LangChain 不必须依赖 LangServe，但在这个指南中，我们会向您展示如何利用 LangServe 部署您的应用。

## 使用 LangChain 构建应用

LangChain 提供了许多模块，可以用来构建基于语言模型的应用程序。这些模块既可以在简单的应用中独立使用，也可以组合在一起以适应更复杂的场景。这种组合由 LangChain 表达式语言（LCEL）提供支持，它定义了一个统一的 Runnable 接口，许多模块都实现了这个接口，从而实现了组件之间的无缝链接。

最简单且常见的链条包括三个要素：

- LLM/ChatModel （LLM/聊天模型）：语言模型是这里的核心推理引擎。要使用 LangChain，您需要了解不同类型的语言模型以及它们的使用方法。
- Prompt Template （提示模板）：它向语言模型提供指令。这直接控制了语言模型的输出，因此理解如何构建提示以及不同的提示策略非常重要。
- Output Parser: （输出解析器）：这些工具将语言模型的原始响应转换成更实用的格式，使得输出易于下游使用。

在这个指南中，我们将分别介绍这三个组件，然后讨论如何将它们组合起来。理解这些概念将有助于您更好地使用和定制 LangChain 应用。大多数 LangChain 应用都允许您配置模型和/或提示，因此了解如何充分利用这一点将大大促进您的工作。

### LLM / ChatModel

语言模型有两种类型：

- `LLM`：这种底层模型接受一个字符串作为输入，并返回一个字符串。
- `ChatModel`：这种底层模型接受一系列消息作为输入，并返回一条消息。

字符串相对简单，但“消息”具体是什么呢？基础消息接口由 BaseMessage 定义，它有两个必要属性：

- `content`：消息的内容，通常是一个字符串。
- `role`：`BaseMessage` 的发送者。

LangChain 提供了几种对象，以便轻松区分不同的角色：

- `HumanMessage`：由人类/用户发送的 `BaseMessage`。
- `AIMessage`：由 AI/助手发送的 `BaseMessage`。
- `SystemMessage`：由系统发送的 `BaseMessage`。
- `FunctionMessage` / `ToolMessage`：包含函数或工具调用输出的 `BaseMessage`。

如果这些角色都不适合，还可以使用 `ChatMessage` 类，在其中手动指定角色。

LangChain 提供了一个通用接口，适用于 `LLM` 和 `ChatModel`s。但理解它们之间的差异对于有效构建特定语言模型的提示非常重要。

调用 `LLM` 或 `ChatModel` 最简单的方式是使用 `.invoke()`，这是所有 LangChain 表达式语言（LCEL）对象的通用同步调用方法：

- `LLM.invoke`：接收一个字符串，返回一个字符串。
- `ChatModel.invoke`：接收一系列 `BaseMessage`，返回一个 `BaseMessage`。

这些方法的输入类型实际上更为通用，但为简化起见，我们可以假设 LLM 只接受字符串，而聊天模型仅接受消息列表。欲了解有关模型调用的更多信息，请查看下方的“深入了解”部分。

接下来，我们来看看如何使用这些不同类型的模型以及它们的不同输入类型。首先，我们导入一个 LLM 和一个聊天模型。

```Python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

llm = OpenAI()
chat_model = ChatOpenAI()
```

 `LLM` 或 `ChatModel` 实质上是配置对象。您可以使用 `temperature` 等参数对它们进行初始化，并进行传递。

```Python
from langchain.schema import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

#### 深入了解

实际上，`LLM.invoke` 和 `ChatModel.invoke` 都支持 `Union[str, List[BaseMessage], PromptValue]` 作为输入类型。`PromptValue` 是一个定义了自己的自定义逻辑的对象，可以将其输入转换为字符串或消息形式。`LLM` 包含将这些输入之一转换为字符串的逻辑，而 `ChatModel` 则包含将它们转换为消息的逻辑。 `LLM` 或 `ChatModel` 接受相同的输入类型，这意味着在大多数链中，您可以直接互换这两者而不会破坏任何东西。当然，思考输入是如何被转换的，以及这可能如何影响模型性能，是非常重要的。要深入了解模型的更多信息，请参阅语言模型部分。

### Prompt templates（提示模板）

大多数基于 LLM 的应用程序并不会直接将用户输入传递给 LLM。它们通常会将用户输入添加到一个更大的文本中，这个文本称为提示模板，它提供了关于当前特定任务的额外上下文信息。

在之前的例子中，我们传给模型的文本包含了生成公司名称的指令。对于我们的应用来说，最理想的情况是用户只需提供公司或产品的描述，而不需要关心如何给模型下指令。

PromptTemplates 正是为了解决这个问题而设计的！它们将从用户输入到格式化完整提示的所有逻辑封装起来。这个过程可以非常简单——比如，上述字符串的提示可能只是：

```Python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")
```

```
What is a good name for a company that makes colorful socks?
```

然而，相比于原始字符串格式化，使用 PromptTemplates 的好处有几个。您可以只对部分变量进行格式化——例如，一次只处理一部分变量。您还可以将它们组合起来，轻松地将不同的模板合并成一个提示。关于这些功能的更多解释，请参阅有关提示的章节以获取更多详情。

PromptTemplates 还可以用来生成消息列表。在这种情况下，提示不仅包含内容的信息，还包括每条消息的信息（例如其角色、在列表中的位置等）。这里最常见的情况是 ChatPromptTemplate 是一个 ChatMessageTemplates 列表。每个 ChatMessageTemplate 包含了如何格式化该 ChatMessage 的指令——它的角色以及内容。让我们来看一下下面的例子：...

```Python
from langchain.prompts.chat import ChatPromptTemplate

template = "You are a helpful assistant that translates {input_language} to {output_language}."
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])

chat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")
```

```
[
    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),
    HumanMessage(content="I love programming.")
]
```

ChatPromptTemplates 也可以通过其他方式构建——有关更多细节，请参阅有关[提示](https://python.langchain.com/docs/modules/model_io/prompts)的章节。

### OutputParsers（输出解析器）

`OutputParsers` 负责将语言模型的原始输出转换成可供下游使用的格式。主要有几种类型的 `OutputParsers`，包括：

- 将 `LLM` 生成的文本转换为结构化信息（如 JSON）。
- 将 `ChatMessage` 转换为纯字符串。
- 将调用返回的额外信息（例如 OpenAI 函数调用的结果）转换为字符串。

有关更多信息，请参阅关于[输出解析器](https://python.langchain.com/docs/modules/model_io/output_parsers)的章节。

在这个入门指南中，我们将编写自己的输出解析器——一个可以将逗号分隔的列表转换成列表的解析器。

```Python
from langchain.schema import BaseOutputParser

class CommaSeparatedListOutputParser(BaseOutputParser):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str):
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

CommaSeparatedListOutputParser().parse("hi, bye")
# >> ['hi', 'bye']
```

### 使用 LCEL 进行组合

现在，我们可以将这些元素组合成一个完整的链条。这个链条会接收输入变量，将它们传递给提示模板以生成一个提示，然后将这个提示传递给语言模型，最后通过一个（可选的）输出解析器处理输出。这是一种将模块化逻辑打包的便捷方式。让我们来看看它的实际应用吧！

```Python
from typing import List

from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import BaseOutputParser

class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str) -> List[str]:
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

template = """You are a helpful assistant who generates comma separated lists.
A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.
ONLY return a comma separated list, and nothing more."""
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])
chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()
chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

需要注意的是，我们使用了 | 语法来连接这些组件。这个 | 语法是由 LangChain 表达式语言（LCEL）支持的，它依赖于所有这些对象实现的通用 Runnable 接口。想要深入了解 LCEL，请阅读这里的相关文档。

## 使用 LangSmith 进行跟踪

假设我们已按照开始的示例设置了环境变量，我们进行的所有模型和链的调用都已自动记录到 LangSmith 中。在 LangSmith 中，我们可以调试和注释我们的应用程序跟踪，然后将它们转换成数据集，用于评估应用程序未来的迭代版本。

您可以查看上述链的跟踪记录，看起来像这样：https://smith.langchain.com/public/09370280-4330-4eb4-a7e8-c91817f6aa13/r

想要了解更多关于 LangSmith 的信息，请访问此[链接]([head here](https://python.langchain.com/docs/langsmith/))。

## 使用 LangServe 提供服务

现在我们已经构建了一个应用程序，接下来需要提供服务。这正是 LangServe 的用途。LangServe 帮助开发者将 LCEL 链作为 REST API 部署。这个库与 FastAPI 集成，并使用 pydantic 进行数据验证。

### Server（服务器）

为了为我们的应用创建一个服务器，我们将创建一个名为 serve.py 的文件，包含以下三个部分：

- 我们的链定义（与上面相同）。
- 我们的 FastAPI 应用。
- 用于提供链服务的路由定义，通过 langserve.add_routes 实现。

```Python
#!/usr/bin/env python
from typing import List

from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema import BaseOutputParser
from langserve import add_routes

# 1. Chain definition

class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str) -> List[str]:
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

template = """You are a helpful assistant who generates comma separated lists.
A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.
ONLY return a comma separated list, and nothing more."""
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])
category_chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()

# 2. App definition
app = FastAPI(
  title="LangChain Server",
  version="1.0",
  description="A simple API server using LangChain's Runnable interfaces",
)

# 3. Adding chain route
add_routes(
    app,
    category_chain,
    path="/category_chain",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

只需这样！执行这个文件后，我们的链应该会在 localhost:8000 上提供服务。

```CLI
python serve.py
```

### Playground（游乐场）


每个 LangServe 服务都自带一个简易的内置 UI，用于配置和调用应用程序，提供流式输出以及对中间步骤的可视化。请访问 http://localhost:8000/category_chain/playground/ 来尝试一下！

### Client（客户端）

现在，让我们设置一个客户端，用于以编程方式与我们的服务进行交互。我们可以轻松地使用 `langserve.RemoteRunnable` 来实现这一点。借助它，我们可以像在客户端运行一样与服务端的链进行交互。 

```Python
from langserve import RemoteRunnable

remote_chain = RemoteRunnable("http://localhost:8000/category_chain/")
remote_chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

想了解更多关于 LangServe 的其他功能，请访问此[链接](https://python.langchain.com/docs/langserve)。


## Reference

- [Quickstart | 🦜️🔗 Langchain](https://python.langchain.com/docs/get_started/quickstart)

## Changelog

- 2023-12-16 16:56 Created by Jiayu