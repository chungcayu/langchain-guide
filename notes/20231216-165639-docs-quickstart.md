# Quickstart

åœ¨è¿™ä¸ªå¿«é€Ÿå…¥é—¨æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨ä»‹ç»å¦‚ä½•ï¼š

- è®¾ç½®å¹¶å¼€å§‹ä½¿ç”¨ LangChainã€LangSmith å’Œ LangServeã€‚
- ä½¿ç”¨ LangChain æœ€åŸºç¡€å’Œå¸¸ç”¨çš„ç»„ä»¶ï¼ŒåŒ…æ‹¬æç¤ºæ¨¡æ¿ã€æ¨¡å‹å’Œè¾“å‡ºè§£æå™¨ã€‚
- åº”ç”¨ LangChain è¡¨è¾¾å¼è¯­è¨€ï¼Œè¿™æ˜¯æ„å»º LangChain çš„åŸºç¡€åè®®ï¼Œå®ƒä¿ƒè¿›äº†ç»„ä»¶é—´çš„é“¾æ¥ã€‚
- åˆ©ç”¨ LangChain æ„å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ç¨‹åºã€‚
- ä½¿ç”¨ LangSmith å¯¹æ‚¨çš„åº”ç”¨ç¨‹åºè¿›è¡Œè·Ÿè¸ªã€‚
- ä½¿ç”¨ LangServe å°†æ‚¨çš„åº”ç”¨éƒ¨ç½²ä¸º REST APIã€‚

## é…ç½®

### ç¯å¢ƒ

ä½¿ç”¨ LangChain é€šå¸¸éœ€è¦ä¸ä¸€ä¸ªæˆ–å¤šä¸ªæ¨¡å‹æä¾›è€…ã€æ•°æ®å­˜å‚¨ã€API ç­‰è¿›è¡Œé›†æˆã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ OpenAI çš„æ¨¡å‹ APIã€‚

### LangSmith

åœ¨ä½¿ç”¨ LangChain æ„å»ºçš„åº”ç”¨ç¨‹åºä¸­ï¼Œé€šå¸¸åŒ…å«å¤šä¸ªæ­¥éª¤å’Œå¤šæ¬¡è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚éšç€åº”ç”¨ç¨‹åºå˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œèƒ½å¤Ÿå‡†ç¡®äº†è§£é“¾æˆ–æ™ºèƒ½ä½“å†…éƒ¨å‘ç”Ÿçš„æƒ…å†µå˜å¾—éå¸¸é‡è¦ã€‚æœ€ä½³çš„æ–¹å¼æ˜¯ä½¿ç”¨ LangSmithã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ LangSmith å¹¶éå¿…éœ€ï¼Œä½†å®ƒéå¸¸æœ‰ç”¨ã€‚å¦‚æœæ‚¨æ‰“ç®—ä½¿ç”¨ LangSmithï¼Œåœ¨ä¸Šè¿°é“¾æ¥æ³¨å†Œåï¼Œè¯·ç¡®ä¿è®¾ç½®æ‚¨çš„ç¯å¢ƒå˜é‡ä»¥å¼€å§‹è®°å½•è·Ÿè¸ªä¿¡æ¯ã€‚

### LangServe

LangServe å¸®åŠ©å¼€å‘è€…å°† LangChain é“¾éƒ¨ç½²ä¸º REST APIã€‚è™½ç„¶ä½¿ç”¨ LangChain ä¸å¿…é¡»ä¾èµ– LangServeï¼Œä½†åœ¨è¿™ä¸ªæŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä¼šå‘æ‚¨å±•ç¤ºå¦‚ä½•åˆ©ç”¨ LangServe éƒ¨ç½²æ‚¨çš„åº”ç”¨ã€‚

## ä½¿ç”¨ LangChain æ„å»ºåº”ç”¨

LangChain æä¾›äº†è®¸å¤šæ¨¡å—ï¼Œå¯ä»¥ç”¨æ¥æ„å»ºåŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºã€‚è¿™äº›æ¨¡å—æ—¢å¯ä»¥åœ¨ç®€å•çš„åº”ç”¨ä¸­ç‹¬ç«‹ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ç»„åˆåœ¨ä¸€èµ·ä»¥é€‚åº”æ›´å¤æ‚çš„åœºæ™¯ã€‚è¿™ç§ç»„åˆç”± LangChain è¡¨è¾¾å¼è¯­è¨€ï¼ˆLCELï¼‰æä¾›æ”¯æŒï¼Œå®ƒå®šä¹‰äº†ä¸€ä¸ªç»Ÿä¸€çš„ Runnable æ¥å£ï¼Œè®¸å¤šæ¨¡å—éƒ½å®ç°äº†è¿™ä¸ªæ¥å£ï¼Œä»è€Œå®ç°äº†ç»„ä»¶ä¹‹é—´çš„æ— ç¼é“¾æ¥ã€‚

æœ€ç®€å•ä¸”å¸¸è§çš„é“¾æ¡åŒ…æ‹¬ä¸‰ä¸ªè¦ç´ ï¼š

- LLM/ChatModel ï¼ˆLLM/èŠå¤©æ¨¡å‹ï¼‰ï¼šè¯­è¨€æ¨¡å‹æ˜¯è¿™é‡Œçš„æ ¸å¿ƒæ¨ç†å¼•æ“ã€‚è¦ä½¿ç”¨ LangChainï¼Œæ‚¨éœ€è¦äº†è§£ä¸åŒç±»å‹çš„è¯­è¨€æ¨¡å‹ä»¥åŠå®ƒä»¬çš„ä½¿ç”¨æ–¹æ³•ã€‚
- Prompt Template ï¼ˆæç¤ºæ¨¡æ¿ï¼‰ï¼šå®ƒå‘è¯­è¨€æ¨¡å‹æä¾›æŒ‡ä»¤ã€‚è¿™ç›´æ¥æ§åˆ¶äº†è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œå› æ­¤ç†è§£å¦‚ä½•æ„å»ºæç¤ºä»¥åŠä¸åŒçš„æç¤ºç­–ç•¥éå¸¸é‡è¦ã€‚
- Output Parser: ï¼ˆè¾“å‡ºè§£æå™¨ï¼‰ï¼šè¿™äº›å·¥å…·å°†è¯­è¨€æ¨¡å‹çš„åŸå§‹å“åº”è½¬æ¢æˆæ›´å®ç”¨çš„æ ¼å¼ï¼Œä½¿å¾—è¾“å‡ºæ˜“äºä¸‹æ¸¸ä½¿ç”¨ã€‚

åœ¨è¿™ä¸ªæŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åˆ†åˆ«ä»‹ç»è¿™ä¸‰ä¸ªç»„ä»¶ï¼Œç„¶åè®¨è®ºå¦‚ä½•å°†å®ƒä»¬ç»„åˆèµ·æ¥ã€‚ç†è§£è¿™äº›æ¦‚å¿µå°†æœ‰åŠ©äºæ‚¨æ›´å¥½åœ°ä½¿ç”¨å’Œå®šåˆ¶ LangChain åº”ç”¨ã€‚å¤§å¤šæ•° LangChain åº”ç”¨éƒ½å…è®¸æ‚¨é…ç½®æ¨¡å‹å’Œ/æˆ–æç¤ºï¼Œå› æ­¤äº†è§£å¦‚ä½•å……åˆ†åˆ©ç”¨è¿™ä¸€ç‚¹å°†å¤§å¤§ä¿ƒè¿›æ‚¨çš„å·¥ä½œã€‚

### LLM / ChatModel

è¯­è¨€æ¨¡å‹æœ‰ä¸¤ç§ç±»å‹ï¼š

- `LLM`ï¼šè¿™ç§åº•å±‚æ¨¡å‹æ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
- `ChatModel`ï¼šè¿™ç§åº•å±‚æ¨¡å‹æ¥å—ä¸€ç³»åˆ—æ¶ˆæ¯ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€æ¡æ¶ˆæ¯ã€‚

å­—ç¬¦ä¸²ç›¸å¯¹ç®€å•ï¼Œä½†â€œæ¶ˆæ¯â€å…·ä½“æ˜¯ä»€ä¹ˆå‘¢ï¼ŸåŸºç¡€æ¶ˆæ¯æ¥å£ç”± BaseMessage å®šä¹‰ï¼Œå®ƒæœ‰ä¸¤ä¸ªå¿…è¦å±æ€§ï¼š

- `content`ï¼šæ¶ˆæ¯çš„å†…å®¹ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
- `role`ï¼š`BaseMessage` çš„å‘é€è€…ã€‚

LangChain æä¾›äº†å‡ ç§å¯¹è±¡ï¼Œä»¥ä¾¿è½»æ¾åŒºåˆ†ä¸åŒçš„è§’è‰²ï¼š

- `HumanMessage`ï¼šç”±äººç±»/ç”¨æˆ·å‘é€çš„ `BaseMessage`ã€‚
- `AIMessage`ï¼šç”± AI/åŠ©æ‰‹å‘é€çš„ `BaseMessage`ã€‚
- `SystemMessage`ï¼šç”±ç³»ç»Ÿå‘é€çš„ `BaseMessage`ã€‚
- `FunctionMessage` / `ToolMessage`ï¼šåŒ…å«å‡½æ•°æˆ–å·¥å…·è°ƒç”¨è¾“å‡ºçš„ `BaseMessage`ã€‚

å¦‚æœè¿™äº›è§’è‰²éƒ½ä¸é€‚åˆï¼Œè¿˜å¯ä»¥ä½¿ç”¨ `ChatMessage` ç±»ï¼Œåœ¨å…¶ä¸­æ‰‹åŠ¨æŒ‡å®šè§’è‰²ã€‚

LangChain æä¾›äº†ä¸€ä¸ªé€šç”¨æ¥å£ï¼Œé€‚ç”¨äº `LLM` å’Œ `ChatModel`sã€‚ä½†ç†è§£å®ƒä»¬ä¹‹é—´çš„å·®å¼‚å¯¹äºæœ‰æ•ˆæ„å»ºç‰¹å®šè¯­è¨€æ¨¡å‹çš„æç¤ºéå¸¸é‡è¦ã€‚

è°ƒç”¨ `LLM` æˆ– `ChatModel` æœ€ç®€å•çš„æ–¹å¼æ˜¯ä½¿ç”¨ `.invoke()`ï¼Œè¿™æ˜¯æ‰€æœ‰ LangChain è¡¨è¾¾å¼è¯­è¨€ï¼ˆLCELï¼‰å¯¹è±¡çš„é€šç”¨åŒæ­¥è°ƒç”¨æ–¹æ³•ï¼š

- `LLM.invoke`ï¼šæ¥æ”¶ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
- `ChatModel.invoke`ï¼šæ¥æ”¶ä¸€ç³»åˆ— `BaseMessage`ï¼Œè¿”å›ä¸€ä¸ª `BaseMessage`ã€‚

è¿™äº›æ–¹æ³•çš„è¾“å…¥ç±»å‹å®é™…ä¸Šæ›´ä¸ºé€šç”¨ï¼Œä½†ä¸ºç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾ LLM åªæ¥å—å­—ç¬¦ä¸²ï¼Œè€ŒèŠå¤©æ¨¡å‹ä»…æ¥å—æ¶ˆæ¯åˆ—è¡¨ã€‚æ¬²äº†è§£æœ‰å…³æ¨¡å‹è°ƒç”¨çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹çš„â€œæ·±å…¥äº†è§£â€éƒ¨åˆ†ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•ä½¿ç”¨è¿™äº›ä¸åŒç±»å‹çš„æ¨¡å‹ä»¥åŠå®ƒä»¬çš„ä¸åŒè¾“å…¥ç±»å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥ä¸€ä¸ª LLM å’Œä¸€ä¸ªèŠå¤©æ¨¡å‹ã€‚

```Python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

llm = OpenAI()
chat_model = ChatOpenAI()
```

 `LLM` æˆ– `ChatModel` å®è´¨ä¸Šæ˜¯é…ç½®å¯¹è±¡ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `temperature` ç­‰å‚æ•°å¯¹å®ƒä»¬è¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶è¿›è¡Œä¼ é€’ã€‚

```Python
from langchain.schema import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

#### æ·±å…¥äº†è§£

å®é™…ä¸Šï¼Œ`LLM.invoke` å’Œ `ChatModel.invoke` éƒ½æ”¯æŒ `Union[str, List[BaseMessage], PromptValue]` ä½œä¸ºè¾“å…¥ç±»å‹ã€‚`PromptValue` æ˜¯ä¸€ä¸ªå®šä¹‰äº†è‡ªå·±çš„è‡ªå®šä¹‰é€»è¾‘çš„å¯¹è±¡ï¼Œå¯ä»¥å°†å…¶è¾“å…¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯å½¢å¼ã€‚`LLM` åŒ…å«å°†è¿™äº›è¾“å…¥ä¹‹ä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²çš„é€»è¾‘ï¼Œè€Œ `ChatModel` åˆ™åŒ…å«å°†å®ƒä»¬è½¬æ¢ä¸ºæ¶ˆæ¯çš„é€»è¾‘ã€‚ `LLM` æˆ– `ChatModel` æ¥å—ç›¸åŒçš„è¾“å…¥ç±»å‹ï¼Œè¿™æ„å‘³ç€åœ¨å¤§å¤šæ•°é“¾ä¸­ï¼Œæ‚¨å¯ä»¥ç›´æ¥äº’æ¢è¿™ä¸¤è€…è€Œä¸ä¼šç ´åä»»ä½•ä¸œè¥¿ã€‚å½“ç„¶ï¼Œæ€è€ƒè¾“å…¥æ˜¯å¦‚ä½•è¢«è½¬æ¢çš„ï¼Œä»¥åŠè¿™å¯èƒ½å¦‚ä½•å½±å“æ¨¡å‹æ€§èƒ½ï¼Œæ˜¯éå¸¸é‡è¦çš„ã€‚è¦æ·±å…¥äº†è§£æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…è¯­è¨€æ¨¡å‹éƒ¨åˆ†ã€‚

### Prompt templatesï¼ˆæç¤ºæ¨¡æ¿ï¼‰

å¤§å¤šæ•°åŸºäº LLM çš„åº”ç”¨ç¨‹åºå¹¶ä¸ä¼šç›´æ¥å°†ç”¨æˆ·è¾“å…¥ä¼ é€’ç»™ LLMã€‚å®ƒä»¬é€šå¸¸ä¼šå°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°ä¸€ä¸ªæ›´å¤§çš„æ–‡æœ¬ä¸­ï¼Œè¿™ä¸ªæ–‡æœ¬ç§°ä¸ºæç¤ºæ¨¡æ¿ï¼Œå®ƒæä¾›äº†å…³äºå½“å‰ç‰¹å®šä»»åŠ¡çš„é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

åœ¨ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¼ ç»™æ¨¡å‹çš„æ–‡æœ¬åŒ…å«äº†ç”Ÿæˆå…¬å¸åç§°çš„æŒ‡ä»¤ã€‚å¯¹äºæˆ‘ä»¬çš„åº”ç”¨æ¥è¯´ï¼Œæœ€ç†æƒ³çš„æƒ…å†µæ˜¯ç”¨æˆ·åªéœ€æä¾›å…¬å¸æˆ–äº§å“çš„æè¿°ï¼Œè€Œä¸éœ€è¦å…³å¿ƒå¦‚ä½•ç»™æ¨¡å‹ä¸‹æŒ‡ä»¤ã€‚

PromptTemplates æ­£æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜è€Œè®¾è®¡çš„ï¼å®ƒä»¬å°†ä»ç”¨æˆ·è¾“å…¥åˆ°æ ¼å¼åŒ–å®Œæ•´æç¤ºçš„æ‰€æœ‰é€»è¾‘å°è£…èµ·æ¥ã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥éå¸¸ç®€å•â€”â€”æ¯”å¦‚ï¼Œä¸Šè¿°å­—ç¬¦ä¸²çš„æç¤ºå¯èƒ½åªæ˜¯ï¼š

```Python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")
```

```
What is a good name for a company that makes colorful socks?
```

ç„¶è€Œï¼Œç›¸æ¯”äºåŸå§‹å­—ç¬¦ä¸²æ ¼å¼åŒ–ï¼Œä½¿ç”¨ PromptTemplates çš„å¥½å¤„æœ‰å‡ ä¸ªã€‚æ‚¨å¯ä»¥åªå¯¹éƒ¨åˆ†å˜é‡è¿›è¡Œæ ¼å¼åŒ–â€”â€”ä¾‹å¦‚ï¼Œä¸€æ¬¡åªå¤„ç†ä¸€éƒ¨åˆ†å˜é‡ã€‚æ‚¨è¿˜å¯ä»¥å°†å®ƒä»¬ç»„åˆèµ·æ¥ï¼Œè½»æ¾åœ°å°†ä¸åŒçš„æ¨¡æ¿åˆå¹¶æˆä¸€ä¸ªæç¤ºã€‚å…³äºè¿™äº›åŠŸèƒ½çš„æ›´å¤šè§£é‡Šï¼Œè¯·å‚é˜…æœ‰å…³æç¤ºçš„ç« èŠ‚ä»¥è·å–æ›´å¤šè¯¦æƒ…ã€‚

PromptTemplates è¿˜å¯ä»¥ç”¨æ¥ç”Ÿæˆæ¶ˆæ¯åˆ—è¡¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæç¤ºä¸ä»…åŒ…å«å†…å®¹çš„ä¿¡æ¯ï¼Œè¿˜åŒ…æ‹¬æ¯æ¡æ¶ˆæ¯çš„ä¿¡æ¯ï¼ˆä¾‹å¦‚å…¶è§’è‰²ã€åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®ç­‰ï¼‰ã€‚è¿™é‡Œæœ€å¸¸è§çš„æƒ…å†µæ˜¯ ChatPromptTemplate æ˜¯ä¸€ä¸ª ChatMessageTemplates åˆ—è¡¨ã€‚æ¯ä¸ª ChatMessageTemplate åŒ…å«äº†å¦‚ä½•æ ¼å¼åŒ–è¯¥ ChatMessage çš„æŒ‡ä»¤â€”â€”å®ƒçš„è§’è‰²ä»¥åŠå†…å®¹ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä¸‹é¢çš„ä¾‹å­ï¼š...

```Python
from langchain.prompts.chat import ChatPromptTemplate

template = "You are a helpful assistant that translates {input_language} to {output_language}."
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])

chat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")
```

```
[
    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),
    HumanMessage(content="I love programming.")
]
```

ChatPromptTemplates ä¹Ÿå¯ä»¥é€šè¿‡å…¶ä»–æ–¹å¼æ„å»ºâ€”â€”æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…æœ‰å…³[æç¤º](https://python.langchain.com/docs/modules/model_io/prompts)çš„ç« èŠ‚ã€‚

### OutputParsersï¼ˆè¾“å‡ºè§£æå™¨ï¼‰

`OutputParsers` è´Ÿè´£å°†è¯­è¨€æ¨¡å‹çš„åŸå§‹è¾“å‡ºè½¬æ¢æˆå¯ä¾›ä¸‹æ¸¸ä½¿ç”¨çš„æ ¼å¼ã€‚ä¸»è¦æœ‰å‡ ç§ç±»å‹çš„ `OutputParsers`ï¼ŒåŒ…æ‹¬ï¼š

- å°† `LLM` ç”Ÿæˆçš„æ–‡æœ¬è½¬æ¢ä¸ºç»“æ„åŒ–ä¿¡æ¯ï¼ˆå¦‚ JSONï¼‰ã€‚
- å°† `ChatMessage` è½¬æ¢ä¸ºçº¯å­—ç¬¦ä¸²ã€‚
- å°†è°ƒç”¨è¿”å›çš„é¢å¤–ä¿¡æ¯ï¼ˆä¾‹å¦‚ OpenAI å‡½æ•°è°ƒç”¨çš„ç»“æœï¼‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚

æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…å…³äº[è¾“å‡ºè§£æå™¨](https://python.langchain.com/docs/modules/model_io/output_parsers)çš„ç« èŠ‚ã€‚

åœ¨è¿™ä¸ªå…¥é—¨æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ç¼–å†™è‡ªå·±çš„è¾“å‡ºè§£æå™¨â€”â€”ä¸€ä¸ªå¯ä»¥å°†é€—å·åˆ†éš”çš„åˆ—è¡¨è½¬æ¢æˆåˆ—è¡¨çš„è§£æå™¨ã€‚

```Python
from langchain.schema import BaseOutputParser

class CommaSeparatedListOutputParser(BaseOutputParser):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str):
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

CommaSeparatedListOutputParser().parse("hi, bye")
# >> ['hi', 'bye']
```

### ä½¿ç”¨ LCEL è¿›è¡Œç»„åˆ

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™äº›å…ƒç´ ç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„é“¾æ¡ã€‚è¿™ä¸ªé“¾æ¡ä¼šæ¥æ”¶è¾“å…¥å˜é‡ï¼Œå°†å®ƒä»¬ä¼ é€’ç»™æç¤ºæ¨¡æ¿ä»¥ç”Ÿæˆä¸€ä¸ªæç¤ºï¼Œç„¶åå°†è¿™ä¸ªæç¤ºä¼ é€’ç»™è¯­è¨€æ¨¡å‹ï¼Œæœ€åé€šè¿‡ä¸€ä¸ªï¼ˆå¯é€‰çš„ï¼‰è¾“å‡ºè§£æå™¨å¤„ç†è¾“å‡ºã€‚è¿™æ˜¯ä¸€ç§å°†æ¨¡å—åŒ–é€»è¾‘æ‰“åŒ…çš„ä¾¿æ·æ–¹å¼ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹å®ƒçš„å®é™…åº”ç”¨å§ï¼

```Python
from typing import List

from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import BaseOutputParser

class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str) -> List[str]:
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

template = """You are a helpful assistant who generates comma separated lists.
A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.
ONLY return a comma separated list, and nothing more."""
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])
chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()
chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† | è¯­æ³•æ¥è¿æ¥è¿™äº›ç»„ä»¶ã€‚è¿™ä¸ª | è¯­æ³•æ˜¯ç”± LangChain è¡¨è¾¾å¼è¯­è¨€ï¼ˆLCELï¼‰æ”¯æŒçš„ï¼Œå®ƒä¾èµ–äºæ‰€æœ‰è¿™äº›å¯¹è±¡å®ç°çš„é€šç”¨ Runnable æ¥å£ã€‚æƒ³è¦æ·±å…¥äº†è§£ LCELï¼Œè¯·é˜…è¯»è¿™é‡Œçš„ç›¸å…³æ–‡æ¡£ã€‚

## ä½¿ç”¨ LangSmith è¿›è¡Œè·Ÿè¸ª

å‡è®¾æˆ‘ä»¬å·²æŒ‰ç…§å¼€å§‹çš„ç¤ºä¾‹è®¾ç½®äº†ç¯å¢ƒå˜é‡ï¼Œæˆ‘ä»¬è¿›è¡Œçš„æ‰€æœ‰æ¨¡å‹å’Œé“¾çš„è°ƒç”¨éƒ½å·²è‡ªåŠ¨è®°å½•åˆ° LangSmith ä¸­ã€‚åœ¨ LangSmith ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒè¯•å’Œæ³¨é‡Šæˆ‘ä»¬çš„åº”ç”¨ç¨‹åºè·Ÿè¸ªï¼Œç„¶åå°†å®ƒä»¬è½¬æ¢æˆæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°åº”ç”¨ç¨‹åºæœªæ¥çš„è¿­ä»£ç‰ˆæœ¬ã€‚

æ‚¨å¯ä»¥æŸ¥çœ‹ä¸Šè¿°é“¾çš„è·Ÿè¸ªè®°å½•ï¼Œçœ‹èµ·æ¥åƒè¿™æ ·ï¼šhttps://smith.langchain.com/public/09370280-4330-4eb4-a7e8-c91817f6aa13/r

æƒ³è¦äº†è§£æ›´å¤šå…³äº LangSmith çš„ä¿¡æ¯ï¼Œè¯·è®¿é—®æ­¤[é“¾æ¥]([head here](https://python.langchain.com/docs/langsmith/))ã€‚

## ä½¿ç”¨ LangServe æä¾›æœåŠ¡

ç°åœ¨æˆ‘ä»¬å·²ç»æ„å»ºäº†ä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œæ¥ä¸‹æ¥éœ€è¦æä¾›æœåŠ¡ã€‚è¿™æ­£æ˜¯ LangServe çš„ç”¨é€”ã€‚LangServe å¸®åŠ©å¼€å‘è€…å°† LCEL é“¾ä½œä¸º REST API éƒ¨ç½²ã€‚è¿™ä¸ªåº“ä¸ FastAPI é›†æˆï¼Œå¹¶ä½¿ç”¨ pydantic è¿›è¡Œæ•°æ®éªŒè¯ã€‚

### Serverï¼ˆæœåŠ¡å™¨ï¼‰

ä¸ºäº†ä¸ºæˆ‘ä»¬çš„åº”ç”¨åˆ›å»ºä¸€ä¸ªæœåŠ¡å™¨ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåä¸º serve.py çš„æ–‡ä»¶ï¼ŒåŒ…å«ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š

- æˆ‘ä»¬çš„é“¾å®šä¹‰ï¼ˆä¸ä¸Šé¢ç›¸åŒï¼‰ã€‚
- æˆ‘ä»¬çš„ FastAPI åº”ç”¨ã€‚
- ç”¨äºæä¾›é“¾æœåŠ¡çš„è·¯ç”±å®šä¹‰ï¼Œé€šè¿‡ langserve.add_routes å®ç°ã€‚

```Python
#!/usr/bin/env python
from typing import List

from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema import BaseOutputParser
from langserve import add_routes

# 1. Chain definition

class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str) -> List[str]:
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

template = """You are a helpful assistant who generates comma separated lists.
A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.
ONLY return a comma separated list, and nothing more."""
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])
category_chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()

# 2. App definition
app = FastAPI(
  title="LangChain Server",
  version="1.0",
  description="A simple API server using LangChain's Runnable interfaces",
)

# 3. Adding chain route
add_routes(
    app,
    category_chain,
    path="/category_chain",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

åªéœ€è¿™æ ·ï¼æ‰§è¡Œè¿™ä¸ªæ–‡ä»¶åï¼Œæˆ‘ä»¬çš„é“¾åº”è¯¥ä¼šåœ¨ localhost:8000 ä¸Šæä¾›æœåŠ¡ã€‚

```CLI
python serve.py
```

### Playgroundï¼ˆæ¸¸ä¹åœºï¼‰


æ¯ä¸ª LangServe æœåŠ¡éƒ½è‡ªå¸¦ä¸€ä¸ªç®€æ˜“çš„å†…ç½® UIï¼Œç”¨äºé…ç½®å’Œè°ƒç”¨åº”ç”¨ç¨‹åºï¼Œæä¾›æµå¼è¾“å‡ºä»¥åŠå¯¹ä¸­é—´æ­¥éª¤çš„å¯è§†åŒ–ã€‚è¯·è®¿é—® http://localhost:8000/category_chain/playground/ æ¥å°è¯•ä¸€ä¸‹ï¼

### Clientï¼ˆå®¢æˆ·ç«¯ï¼‰

ç°åœ¨ï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªå®¢æˆ·ç«¯ï¼Œç”¨äºä»¥ç¼–ç¨‹æ–¹å¼ä¸æˆ‘ä»¬çš„æœåŠ¡è¿›è¡Œäº¤äº’ã€‚æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ `langserve.RemoteRunnable` æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å€ŸåŠ©å®ƒï¼Œæˆ‘ä»¬å¯ä»¥åƒåœ¨å®¢æˆ·ç«¯è¿è¡Œä¸€æ ·ä¸æœåŠ¡ç«¯çš„é“¾è¿›è¡Œäº¤äº’ã€‚ 

```Python
from langserve import RemoteRunnable

remote_chain = RemoteRunnable("http://localhost:8000/category_chain/")
remote_chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

æƒ³äº†è§£æ›´å¤šå…³äº LangServe çš„å…¶ä»–åŠŸèƒ½ï¼Œè¯·è®¿é—®æ­¤[é“¾æ¥](https://python.langchain.com/docs/langserve)ã€‚


## Reference

- [Quickstart | ğŸ¦œï¸ğŸ”— Langchain](https://python.langchain.com/docs/get_started/quickstart)

## Changelog

- 2023-12-16 16:56 Created by Jiayu